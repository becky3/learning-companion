"""Slack ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©
ä»•æ§˜: docs/specs/f1-chat.md, docs/specs/f2-feed-collection.md, docs/specs/f3-user-profiling.md, docs/specs/f4-topic-recommend.md, docs/specs/f6-auto-reply.md, docs/specs/f7-bot-status.md, docs/specs/f8-thread-support.md
"""

from __future__ import annotations

import asyncio
import csv
import io
import logging
import re
import socket
from datetime import datetime
from typing import TYPE_CHECKING, Literal
from urllib.parse import urlparse
from zoneinfo import ZoneInfo

import httpx
from slack_bolt.async_app import AsyncApp

from src.services.chat import ChatService
from src.services.topic_recommender import TopicRecommender
from src.services.user_profiler import UserProfiler

if TYPE_CHECKING:
    from sqlalchemy.ext.asyncio import AsyncSession, async_sessionmaker

    from src.services.feed_collector import FeedCollector

logger = logging.getLogger(__name__)

_PROFILE_KEYWORDS = ("ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«", "ãƒ—ãƒ­ãƒ•ã‚£ãƒ¼ãƒ«", "profile")
_TOPIC_KEYWORDS = ("ãŠã™ã™ã‚", "ãƒˆãƒ”ãƒƒã‚¯", "ä½•ã‚’å­¦ã¶", "ä½•å­¦ã¶", "å­¦ç¿’ææ¡ˆ", "recommend")
_DELIVER_KEYWORDS = ("deliver",)
_FEED_KEYWORDS = ("feed",)
_STATUS_KEYWORDS = ("status", "info")

# èµ·å‹•æ™‚åˆ»ï¼ˆmain.py ã‹ã‚‰è¨­å®šã•ã‚Œã‚‹ï¼‰
BOT_START_TIME: datetime | None = None


def strip_mention(text: str) -> str:
    """ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³éƒ¨åˆ† (<@U...>) ã‚’é™¤å»ã™ã‚‹."""
    return re.sub(r"<@[A-Za-z0-9]+>\s*", "", text).strip()


def _parse_feed_command(text: str) -> tuple[str, list[str], str]:
    """feedã‚³ãƒãƒ³ãƒ‰ã‚’è§£æã™ã‚‹.

    Args:
        text: "feed add https://example.com/rss Python" ã®ã‚ˆã†ãªã‚³ãƒãƒ³ãƒ‰æ–‡å­—åˆ—

    Returns:
        (ã‚µãƒ–ã‚³ãƒãƒ³ãƒ‰, URLãƒªã‚¹ãƒˆ, ã‚«ãƒ†ã‚´ãƒªå) ã®ã‚¿ãƒ—ãƒ«
        ã‚«ãƒ†ã‚´ãƒªåã¯ add ã®å ´åˆã®ã¿ä½¿ç”¨ã•ã‚Œã‚‹ãŒã€å…¨ã‚³ãƒãƒ³ãƒ‰ã§è§£æã•ã‚Œã‚‹ã€‚ã‚«ãƒ†ã‚´ãƒªãƒˆãƒ¼ã‚¯ãƒ³ãŒç„¡ã„å ´åˆã¯ã€Œä¸€èˆ¬ã€ã¨ãªã‚‹ã€‚
    """
    tokens = text.split()
    if len(tokens) < 2:
        return ("", [], "")

    subcommand = tokens[1].lower()
    urls: list[str] = []
    category_tokens: list[str] = []

    for token in tokens[2:]:
        # Slackã¯ URL ã‚’ <https://...|label> å½¢å¼ã«å¤‰æ›ã™ã‚‹ãŸã‚é™¤å»
        cleaned = token.strip("<>")
        if "|" in cleaned:
            cleaned = cleaned.split("|")[0]

        if cleaned.startswith("http://") or cleaned.startswith("https://"):
            parsed_url = urlparse(cleaned)
            if parsed_url.netloc:
                urls.append(cleaned)
            # ãƒ‰ãƒ¡ã‚¤ãƒ³ãªã—ã®ä¸æ­£URLã¯ç„¡è¦–ï¼ˆã‚«ãƒ†ã‚´ãƒªã«ã‚‚è¿½åŠ ã—ãªã„ï¼‰
        elif cleaned.startswith("--"):
            # ãƒ•ãƒ©ã‚°å¼•æ•°ï¼ˆ--skip-summary ãªã©ï¼‰ã¯ã‚«ãƒ†ã‚´ãƒªã«å«ã‚ãªã„
            pass
        else:
            category_tokens.append(token)

    category = " ".join(category_tokens) if category_tokens else "ä¸€èˆ¬"
    return (subcommand, urls, category)


def _format_uptime(seconds: float) -> str:
    """ç¨¼åƒæ™‚é–“ã‚’ã€ŒNæ™‚é–“Måˆ†ã€å½¢å¼ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã™ã‚‹."""
    total_minutes = int(seconds) // 60
    hours = total_minutes // 60
    minutes = total_minutes % 60
    if hours > 0:
        return f"{hours}æ™‚é–“{minutes}åˆ†"
    return f"{minutes}åˆ†"


def _build_status_message(timezone: str, env_name: str) -> str:
    """ãƒœãƒƒãƒˆã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ§‹ç¯‰ã™ã‚‹ (F7)."""
    hostname = socket.gethostname()
    now = datetime.now(tz=ZoneInfo(timezone))

    lines = ["\U0001f916 ãƒœãƒƒãƒˆã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹", f"ãƒ›ã‚¹ãƒˆ: {hostname}"]

    if env_name:
        lines.append(f"ç’°å¢ƒ: {env_name}")

    if BOT_START_TIME is not None:
        start_str = BOT_START_TIME.strftime("%Y-%m-%d %H:%M:%S %Z")
        uptime = now - BOT_START_TIME
        uptime_str = _format_uptime(uptime.total_seconds())
        lines.append(f"èµ·å‹•: {start_str}ï¼ˆç¨¼åƒ {uptime_str}ï¼‰")

    return "\n".join(lines)


async def _handle_feed_add(
    collector: FeedCollector, urls: list[str], category: str
) -> str:
    """ãƒ•ã‚£ãƒ¼ãƒ‰è¿½åŠ å‡¦ç†."""
    if not urls:
        return "ã‚¨ãƒ©ãƒ¼: URLã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚\nä¾‹: `@bot feed add https://example.com/rss Python`"

    results: list[str] = []
    for url in urls:
        try:
            feed = await collector.add_feed(url, url, category)
            results.append(f"âœ… {feed.url} ã‚’è¿½åŠ ã—ã¾ã—ãŸï¼ˆã‚«ãƒ†ã‚´ãƒª: {feed.category}ï¼‰")
        except ValueError as e:
            results.append(f"âŒ {url}: {e}")
        except Exception:
            logger.exception("Failed to add feed: %s", url)
            results.append(f"âŒ {url}: è¿½åŠ ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")

    return "\n".join(results)


async def _handle_feed_list(collector: FeedCollector) -> str:
    """ãƒ•ã‚£ãƒ¼ãƒ‰ä¸€è¦§è¡¨ç¤ºå‡¦ç†."""
    enabled, disabled = await collector.list_feeds()

    if not enabled and not disabled:
        return "ãƒ•ã‚£ãƒ¼ãƒ‰ãŒç™»éŒ²ã•ã‚Œã¦ã„ã¾ã›ã‚“"

    lines: list[str] = []
    if enabled:
        lines.append("*æœ‰åŠ¹ãªãƒ•ã‚£ãƒ¼ãƒ‰*")
        for feed in enabled:
            lines.append(f"â€¢ {feed.url} â€” {feed.name}")
    else:
        lines.append("æœ‰åŠ¹ãªãƒ•ã‚£ãƒ¼ãƒ‰ã¯ã‚ã‚Šã¾ã›ã‚“")

    if disabled:
        lines.append("\n*ç„¡åŠ¹ãªãƒ•ã‚£ãƒ¼ãƒ‰*")
        for feed in disabled:
            lines.append(f"â€¢ {feed.url} â€” {feed.name}")

    return "\n".join(lines)


async def _handle_feed_delete(collector: FeedCollector, urls: list[str]) -> str:
    """ãƒ•ã‚£ãƒ¼ãƒ‰å‰Šé™¤å‡¦ç†."""
    if not urls:
        return "ã‚¨ãƒ©ãƒ¼: URLã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚\nä¾‹: `@bot feed delete https://example.com/rss`"

    results: list[str] = []
    for url in urls:
        try:
            await collector.delete_feed(url)
            results.append(f"âœ… {url} ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
        except ValueError as e:
            results.append(f"âŒ {url}: {e}")
        except Exception:
            logger.exception("Failed to delete feed: %s", url)
            results.append(f"âŒ {url}: å‰Šé™¤ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")

    return "\n".join(results)


async def _handle_feed_enable(collector: FeedCollector, urls: list[str]) -> str:
    """ãƒ•ã‚£ãƒ¼ãƒ‰æœ‰åŠ¹åŒ–å‡¦ç†."""
    if not urls:
        return "ã‚¨ãƒ©ãƒ¼: URLã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚\nä¾‹: `@bot feed enable https://example.com/rss`"

    results: list[str] = []
    for url in urls:
        try:
            await collector.enable_feed(url)
            results.append(f"âœ… {url} ã‚’æœ‰åŠ¹åŒ–ã—ã¾ã—ãŸ")
        except ValueError as e:
            results.append(f"âŒ {url}: {e}")
        except Exception:
            logger.exception("Failed to enable feed: %s", url)
            results.append(f"âŒ {url}: æœ‰åŠ¹åŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")

    return "\n".join(results)


async def _handle_feed_disable(collector: FeedCollector, urls: list[str]) -> str:
    """ãƒ•ã‚£ãƒ¼ãƒ‰ç„¡åŠ¹åŒ–å‡¦ç†."""
    if not urls:
        return "ã‚¨ãƒ©ãƒ¼: URLã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚\nä¾‹: `@bot feed disable https://example.com/rss`"

    results: list[str] = []
    for url in urls:
        try:
            await collector.disable_feed(url)
            results.append(f"âœ… {url} ã‚’ç„¡åŠ¹åŒ–ã—ã¾ã—ãŸ")
        except ValueError as e:
            results.append(f"âŒ {url}: {e}")
        except Exception:
            logger.exception("Failed to disable feed: %s", url)
            results.append(f"âŒ {url}: ç„¡åŠ¹åŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")

    return "\n".join(results)


async def _download_and_parse_csv(
    files: list[dict[str, object]] | None,
    bot_token: str,
) -> tuple[list[dict[str, str]], str | None]:
    """CSVæ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œè¨¼ãƒ»ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ»ãƒ‘ãƒ¼ã‚¹ã™ã‚‹.

    Returns:
        (è¡Œãƒªã‚¹ãƒˆ, ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸) ã®ã‚¿ãƒ—ãƒ«ã€‚
        æˆåŠŸæ™‚ã¯ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒNoneã€å¤±æ•—æ™‚ã¯è¡Œãƒªã‚¹ãƒˆãŒç©ºã€‚
    """
    if not files:
        return ([], (
            "ã‚¨ãƒ©ãƒ¼: CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ·»ä»˜ã—ã¦ãã ã•ã„ã€‚\n"
            "ä½¿ç”¨æ–¹æ³•: `@bot feed import` ã¾ãŸã¯ `@bot feed replace` ã«CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ·»ä»˜\n"
            "CSVå½¢å¼: `url,name,category`"
        ))

    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
    csv_file = None
    for f in files:
        mimetype = str(f.get("mimetype", ""))
        name = str(f.get("name", ""))
        if mimetype == "text/csv" or name.endswith(".csv"):
            csv_file = f
            break

    if not csv_file:
        return ([], (
            "ã‚¨ãƒ©ãƒ¼: CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\n"
            "CSVå½¢å¼ã®ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ.csvï¼‰ã‚’æ·»ä»˜ã—ã¦ãã ã•ã„ã€‚"
        ))

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºæ¤œè¨¼ï¼ˆæœ€å¤§1MBï¼‰
    max_file_size = 1 * 1024 * 1024  # 1MB
    file_size = csv_file.get("size", 0)
    if isinstance(file_size, int) and file_size > max_file_size:
        return ([], f"ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãã™ãã¾ã™ï¼ˆæœ€å¤§1MBã€å®Ÿéš›: {file_size // 1024}KBï¼‰")

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    url_private = csv_file.get("url_private")
    if not url_private or not isinstance(url_private, str):
        return ([], "ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰URLãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")

    # url_private_download ã‚’å„ªå…ˆçš„ã«ä½¿ç”¨ï¼ˆã‚ˆã‚Šç¢ºå®Ÿï¼‰
    download_url = csv_file.get("url_private_download") or url_private
    if not isinstance(download_url, str):
        download_url = url_private

    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.get(
                download_url,
                headers={"Authorization": f"Bearer {bot_token}"},
                follow_redirects=False,
            )
            # 302ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã®å ´åˆã¯èªè¨¼ã‚¨ãƒ©ãƒ¼
            if response.status_code == 302:
                logger.error("File download redirected - auth may have failed")
                return ([], "ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆèªè¨¼ã‚¨ãƒ©ãƒ¼ï¼‰ã€‚Botæ¨©é™ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            response.raise_for_status()
            content = response.text
    except httpx.HTTPError as e:
        logger.exception("Failed to download CSV file")
        return ([], f"ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    # CSVã‚’ãƒ‘ãƒ¼ã‚¹
    try:
        reader = csv.DictReader(io.StringIO(content))
        # ãƒ˜ãƒƒãƒ€ãƒ¼æ¤œè¨¼
        fieldnames = reader.fieldnames or []
        if "url" not in fieldnames or "name" not in fieldnames:
            return ([], (
                "ã‚¨ãƒ©ãƒ¼: CSVãƒ˜ãƒƒãƒ€ãƒ¼ãŒä¸æ­£ã§ã™ã€‚\n"
                "`url,name,category` ã®å½¢å¼ã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚\n"
                f"æ¤œå‡ºã•ã‚ŒãŸãƒ˜ãƒƒãƒ€ãƒ¼: {', '.join(fieldnames)}"
            ))

        rows = list(reader)
    except csv.Error as e:
        return ([], f"ã‚¨ãƒ©ãƒ¼: CSVã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    if not rows:
        return ([], "ã‚¨ãƒ©ãƒ¼: CSVã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

    return (rows, None)


async def _import_feeds_from_rows(
    collector: FeedCollector,
    rows: list[dict[str, str]],
) -> tuple[int, list[str]]:
    """CSVã®è¡Œãƒªã‚¹ãƒˆã‹ã‚‰ãƒ•ã‚£ãƒ¼ãƒ‰ã‚’ç™»éŒ²ã™ã‚‹.

    Returns:
        (æˆåŠŸä»¶æ•°, ã‚¨ãƒ©ãƒ¼ãƒªã‚¹ãƒˆ) ã®ã‚¿ãƒ—ãƒ«
    """
    success_count = 0
    errors: list[str] = []

    for line_number, row in enumerate(rows, start=2):  # ãƒ˜ãƒƒãƒ€ãƒ¼è¡ŒãŒ1è¡Œç›®ãªã®ã§2ã‹ã‚‰é–‹å§‹
        url = (row.get("url") or "").strip()
        name = (row.get("name") or "").strip()
        category = (row.get("category") or "").strip() or "ä¸€èˆ¬"

        if not url or not name:
            errors.append(f"è¡Œ{line_number}: url ã¾ãŸã¯ name ãŒç©ºã§ã™")
            continue

        # URLã®å½¢å¼ã‚’æ¤œè¨¼
        parsed = urlparse(url)
        if parsed.scheme not in ("http", "https") or not parsed.netloc:
            errors.append(f"è¡Œ{line_number}: ç„¡åŠ¹ãªURLå½¢å¼ã§ã™ï¼ˆ{url}ï¼‰")
            continue

        try:
            await collector.add_feed(url, name, category)
            success_count += 1
        except ValueError as e:
            errors.append(f"è¡Œ{line_number}: {e}")
        except Exception:
            logger.exception("Failed to add feed: %s", url)
            errors.append(f"è¡Œ{line_number}: è¿½åŠ ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")

    return (success_count, errors)


def _format_error_details(errors: list[str]) -> list[str]:
    """ã‚¨ãƒ©ãƒ¼è©³ç´°ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã™ã‚‹."""
    lines: list[str] = []
    if errors:
        lines.append("\n*ã‚¨ãƒ©ãƒ¼è©³ç´°:*")
        for error in errors[:10]:
            lines.append(f"  â€¢ {error}")
        if len(errors) > 10:
            lines.append(f"  ...ä»– {len(errors) - 10}ä»¶")
    return lines


async def _handle_feed_import(
    collector: FeedCollector,
    files: list[dict[str, object]] | None,
    bot_token: str,
) -> str:
    """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ•ã‚£ãƒ¼ãƒ‰ã‚’ä¸€æ‹¬ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹."""
    rows, error = await _download_and_parse_csv(files, bot_token)
    if error is not None:
        return error

    success_count, errors = await _import_feeds_from_rows(collector, rows)

    # çµæœã‚µãƒãƒªãƒ¼ã‚’ä½œæˆ
    result_lines = [
        "*ãƒ•ã‚£ãƒ¼ãƒ‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†*",
        f"âœ… æˆåŠŸ: {success_count}ä»¶",
        f"âŒ å¤±æ•—: {len(errors)}ä»¶",
    ]
    result_lines.extend(_format_error_details(errors))

    return "\n".join(result_lines)


async def _handle_feed_replace(
    collector: FeedCollector,
    files: list[dict[str, object]] | None,
    bot_token: str,
) -> str:
    """CSVãƒ•ã‚¡ã‚¤ãƒ«ã§å…¨ãƒ•ã‚£ãƒ¼ãƒ‰ã‚’ç½®æ›ã™ã‚‹ï¼ˆå…¨å‰Šé™¤â†’å†ç™»éŒ²ï¼‰."""
    rows, error = await _download_and_parse_csv(files, bot_token)
    if error is not None:
        return error

    # å…¨ãƒ•ã‚£ãƒ¼ãƒ‰å‰Šé™¤
    try:
        deleted_count = await collector.delete_all_feeds()
    except Exception:
        logger.exception("Failed to delete all feeds in replace")
        return (
            "*ãƒ•ã‚£ãƒ¼ãƒ‰ç½®æ›ã‚¨ãƒ©ãƒ¼*\n"
            "ğŸ—‘ï¸ æ—¢å­˜ãƒ•ã‚£ãƒ¼ãƒ‰ã®å‰Šé™¤ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚\n"
            "âŒ ãƒ•ã‚£ãƒ¼ãƒ‰ã®ç½®æ›ã‚’å®Œäº†ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
        )

    # CSVã‹ã‚‰ãƒ•ã‚£ãƒ¼ãƒ‰ã‚’ç™»éŒ²
    try:
        success_count, errors = await _import_feeds_from_rows(collector, rows)
    except Exception:
        logger.exception("Failed to import feeds after delete_all in replace")
        return (
            "*ãƒ•ã‚£ãƒ¼ãƒ‰ç½®æ›ã‚¨ãƒ©ãƒ¼*\n"
            f"ğŸ—‘ï¸ å‰Šé™¤: {deleted_count}ä»¶ï¼ˆæ—¢å­˜ãƒ•ã‚£ãƒ¼ãƒ‰ï¼‰\n"
            "âŒ ã‚¤ãƒ³ãƒãƒ¼ãƒˆä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"
        )

    # çµæœã‚µãƒãƒªãƒ¼ã‚’ä½œæˆ
    result_lines = [
        "*ãƒ•ã‚£ãƒ¼ãƒ‰ç½®æ›å®Œäº†*",
        f"ğŸ—‘ï¸ å‰Šé™¤: {deleted_count}ä»¶ï¼ˆæ—¢å­˜ãƒ•ã‚£ãƒ¼ãƒ‰ï¼‰",
        f"âœ… ç™»éŒ²æˆåŠŸ: {success_count}ä»¶",
        f"âŒ ç™»éŒ²å¤±æ•—: {len(errors)}ä»¶",
    ]
    result_lines.extend(_format_error_details(errors))

    return "\n".join(result_lines)


async def _handle_feed_export(
    collector: FeedCollector,
    slack_client: object,
    channel: str,
    thread_ts: str,
) -> str:
    """å…¨ãƒ•ã‚£ãƒ¼ãƒ‰ã‚’CSVå½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹."""
    feeds = await collector.get_all_feeds()

    if not feeds:
        return "ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹ãƒ•ã‚£ãƒ¼ãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"

    # CSVæ–‡å­—åˆ—ã‚’ç”Ÿæˆ
    def _sanitize_csv_field(value: str) -> str:
        """CSVã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³å¯¾ç­–: å…ˆé ­ãŒå±é™ºãªæ–‡å­—ã®å ´åˆã«ã‚·ãƒ³ã‚°ãƒ«ã‚¯ã‚©ãƒ¼ãƒˆã‚’ä»˜ä¸."""
        if value and value[0] in ("=", "+", "-", "@"):
            return f"'{value}"
        return value

    output = io.StringIO()
    writer = csv.writer(output)
    writer.writerow(["url", "name", "category"])
    for feed in feeds:
        writer.writerow([
            feed.url,
            _sanitize_csv_field(feed.name),
            _sanitize_csv_field(feed.category),
        ])
    csv_content = output.getvalue()

    # Slackã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    try:
        await slack_client.files_upload_v2(  # type: ignore[attr-defined]
            channel=channel,
            thread_ts=thread_ts,
            content=csv_content,
            filename="feeds.csv",
            initial_comment=f"ãƒ•ã‚£ãƒ¼ãƒ‰ä¸€è¦§ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ã¾ã—ãŸï¼ˆ{len(feeds)}ä»¶ï¼‰",
        )
    except Exception as e:
        error_msg = str(e)
        if "missing_scope" in error_msg or "not_allowed_token_type" in error_msg:
            logger.error("File upload failed due to missing scope: %s", e)
            return (
                "ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸã€‚\n"
                "Slack Appã« `files:write` ã‚¹ã‚³ãƒ¼ãƒ—ã®è¿½åŠ ãŒå¿…è¦ã§ã™ã€‚"
            )
        logger.exception("Failed to upload CSV file")
        return f"ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}"

    return ""


def register_handlers(
    app: AsyncApp,
    chat_service: ChatService,
    user_profiler: UserProfiler | None = None,
    topic_recommender: TopicRecommender | None = None,
    collector: FeedCollector | None = None,
    session_factory: async_sessionmaker[AsyncSession] | None = None,
    slack_client: object | None = None,
    channel_id: str | None = None,
    max_articles_per_feed: int = 10,
    feed_card_layout: Literal["vertical", "horizontal"] = "horizontal",
    auto_reply_channels: list[str] | None = None,
    bot_token: str | None = None,
    timezone: str = "Asia/Tokyo",
    env_name: str = "",
) -> None:
    """app_mention ãŠã‚ˆã³ message ãƒãƒ³ãƒ‰ãƒ©ã‚’ç™»éŒ²ã™ã‚‹."""

    async def _process_message(
        user_id: str,
        cleaned_text: str,
        thread_ts: str,
        say: object,
        files: list[dict[str, object]] | None = None,
        channel: str = "",
        is_in_thread: bool = False,
        current_ts: str = "",
    ) -> None:
        """å…±é€šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆapp_mention / message å…±ç”¨ï¼‰."""
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒãƒ³ãƒ‰ (F7)
        if cleaned_text.lower().strip() in _STATUS_KEYWORDS:
            response_text = _build_status_message(timezone, env_name)
            await say(text=response_text, thread_ts=thread_ts)  # type: ignore[operator]
            return

        # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ (F3-AC4, F6-AC4)
        if user_profiler is not None and any(
            kw in cleaned_text.lower() for kw in _PROFILE_KEYWORDS
        ):
            profile_text = await user_profiler.get_profile(user_id)
            if profile_text:
                await say(text=profile_text, thread_ts=thread_ts)  # type: ignore[operator]
            else:
                await say(  # type: ignore[operator]
                    text="ã¾ã ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ä¼šè©±ã‚’ç¶šã‘ã‚‹ã¨è‡ªå‹•çš„ã«è¨˜éŒ²ã•ã‚Œã¾ã™ï¼",
                    thread_ts=thread_ts,
                )
            return

        # feedã‚³ãƒãƒ³ãƒ‰ (F2-AC7, F6-AC4)
        lower_text = cleaned_text.lower().lstrip()
        if collector is not None and any(
            re.match(rf"^{re.escape(kw)}\b", lower_text) for kw in _FEED_KEYWORDS
        ):
            subcommand, urls, category = _parse_feed_command(cleaned_text)

            if subcommand == "add":
                response_text = await _handle_feed_add(collector, urls, category)
            elif subcommand == "list":
                response_text = await _handle_feed_list(collector)
            elif subcommand == "delete":
                response_text = await _handle_feed_delete(collector, urls)
            elif subcommand == "enable":
                response_text = await _handle_feed_enable(collector, urls)
            elif subcommand == "disable":
                response_text = await _handle_feed_disable(collector, urls)
            elif subcommand == "import":
                if not bot_token:
                    response_text = "ã‚¨ãƒ©ãƒ¼: Bot TokenãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
                else:
                    response_text = await _handle_feed_import(
                        collector, files, bot_token
                    )
            elif subcommand == "replace":
                if not bot_token:
                    response_text = "ã‚¨ãƒ©ãƒ¼: Bot TokenãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
                else:
                    response_text = await _handle_feed_replace(
                        collector, files, bot_token
                    )
            elif subcommand == "export":
                if slack_client is not None and channel:
                    response_text = await _handle_feed_export(
                        collector, slack_client, channel, thread_ts
                    )
                    if response_text:
                        await say(text=response_text, thread_ts=thread_ts)  # type: ignore[operator]
                else:
                    await say(  # type: ignore[operator]
                        text="ã‚¨ãƒ©ãƒ¼: Slacké€£æºã®è¨­å®šãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚",
                        thread_ts=thread_ts,
                    )
                return
            elif subcommand == "collect":
                # feed collect --skip-summary
                if "--skip-summary" in cleaned_text.lower():
                    if (
                        session_factory is not None
                        and slack_client is not None
                        and channel_id is not None
                    ):
                        from src.scheduler.jobs import daily_collect_and_deliver

                        try:
                            await say(text="è¦ç´„ã‚¹ã‚­ãƒƒãƒ—åé›†ã‚’é–‹å§‹ã—ã¾ã™...", thread_ts=thread_ts)  # type: ignore[operator]
                            feed_count, article_count = await daily_collect_and_deliver(
                                collector, session_factory, slack_client, channel_id,
                                max_articles_per_feed=max_articles_per_feed,
                                layout=feed_card_layout,
                                skip_summary=True,
                            )
                            await say(  # type: ignore[operator]
                                text=f"è¦ç´„ã‚¹ã‚­ãƒƒãƒ—åé›†ãŒå®Œäº†ã—ã¾ã—ãŸ\nåé›†ãƒ•ã‚£ãƒ¼ãƒ‰æ•°: {feed_count}\nåé›†è¨˜äº‹æ•°: {article_count}",
                                thread_ts=thread_ts,
                            )
                        except Exception:
                            logger.exception("Failed to collect feeds with skip-summary")
                            await say(  # type: ignore[operator]
                                text="è¦ç´„ã‚¹ã‚­ãƒƒãƒ—åé›†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚",
                                thread_ts=thread_ts,
                            )
                    else:
                        await say(  # type: ignore[operator]
                            text="ã‚¨ãƒ©ãƒ¼: é…ä¿¡è¨­å®šãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚",
                            thread_ts=thread_ts,
                        )
                else:
                    response_text = (
                        "ä½¿ç”¨æ–¹æ³•:\n"
                        "â€¢ `@bot feed collect --skip-summary` â€” è¦ç´„ãªã—ä¸€æ‹¬åé›†"
                    )
                    await say(text=response_text, thread_ts=thread_ts)  # type: ignore[operator]
                return
            elif subcommand == "test":
                if (
                    session_factory is not None
                    and slack_client is not None
                    and channel_id is not None
                ):
                    from src.scheduler.jobs import feed_test_deliver

                    try:
                        await say(text="ãƒ†ã‚¹ãƒˆé…ä¿¡ã‚’é–‹å§‹ã—ã¾ã™...", thread_ts=thread_ts)  # type: ignore[operator]
                        await feed_test_deliver(
                            session_factory=session_factory,
                            slack_client=slack_client,
                            channel_id=channel_id,
                            layout=feed_card_layout,
                        )
                        await say(text="ãƒ†ã‚¹ãƒˆé…ä¿¡ãŒå®Œäº†ã—ã¾ã—ãŸ", thread_ts=thread_ts)  # type: ignore[operator]
                    except Exception:
                        logger.exception("Failed to run feed test delivery")
                        await say(  # type: ignore[operator]
                            text="ãƒ†ã‚¹ãƒˆé…ä¿¡ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚",
                            thread_ts=thread_ts,
                        )
                else:
                    response_text = "ã‚¨ãƒ©ãƒ¼: é…ä¿¡è¨­å®šãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚"
                    await say(text=response_text, thread_ts=thread_ts)  # type: ignore[operator]
                return
            else:
                response_text = (
                    "ä½¿ç”¨æ–¹æ³•:\n"
                    "â€¢ `@bot feed add <URL> [ã‚«ãƒ†ã‚´ãƒª]` â€” ãƒ•ã‚£ãƒ¼ãƒ‰è¿½åŠ \n"
                    "â€¢ `@bot feed list` â€” ãƒ•ã‚£ãƒ¼ãƒ‰ä¸€è¦§\n"
                    "â€¢ `@bot feed delete <URL>` â€” ãƒ•ã‚£ãƒ¼ãƒ‰å‰Šé™¤\n"
                    "â€¢ `@bot feed enable <URL>` â€” ãƒ•ã‚£ãƒ¼ãƒ‰æœ‰åŠ¹åŒ–\n"
                    "â€¢ `@bot feed disable <URL>` â€” ãƒ•ã‚£ãƒ¼ãƒ‰ç„¡åŠ¹åŒ–\n"
                    "â€¢ `@bot feed import` + CSVæ·»ä»˜ â€” ãƒ•ã‚£ãƒ¼ãƒ‰ä¸€æ‹¬ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n"
                    "â€¢ `@bot feed replace` + CSVæ·»ä»˜ â€” ãƒ•ã‚£ãƒ¼ãƒ‰ä¸€æ‹¬ç½®æ›\n"
                    "â€¢ `@bot feed export` â€” ãƒ•ã‚£ãƒ¼ãƒ‰ä¸€è¦§ã‚’CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ\n"
                    "â€¢ `@bot feed collect --skip-summary` â€” è¦ç´„ãªã—ä¸€æ‹¬åé›†\n"
                    "â€¢ `@bot feed test` â€” ãƒ†ã‚¹ãƒˆé…ä¿¡ï¼ˆä¸Šä½3ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ»å„5ä»¶ï¼‰\n"
                    "â€» URLãƒ»ã‚«ãƒ†ã‚´ãƒªã¯è¤‡æ•°æŒ‡å®šå¯èƒ½ï¼ˆã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šï¼‰"
                )

            await say(text=response_text, thread_ts=thread_ts)  # type: ignore[operator]
            return

        # é…ä¿¡ãƒ†ã‚¹ãƒˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ (F2)
        if (
            collector is not None
            and session_factory is not None
            and slack_client is not None
            and channel_id is not None
            and any(kw in cleaned_text.lower() for kw in _DELIVER_KEYWORDS)
        ):
            from src.scheduler.jobs import daily_collect_and_deliver

            try:
                await say(text="é…ä¿¡ã‚’é–‹å§‹ã—ã¾ã™...", thread_ts=thread_ts)  # type: ignore[operator]
                await daily_collect_and_deliver(
                    collector, session_factory, slack_client, channel_id,
                    max_articles_per_feed=max_articles_per_feed,
                    layout=feed_card_layout,
                )
                await say(text="é…ä¿¡ãŒå®Œäº†ã—ã¾ã—ãŸ", thread_ts=thread_ts)  # type: ignore[operator]
            except Exception:
                logger.exception("Failed to run manual delivery")
                await say(  # type: ignore[operator]
                    text="é…ä¿¡ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚",
                    thread_ts=thread_ts,
                )
            return

        # ãƒˆãƒ”ãƒƒã‚¯ææ¡ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ (F4, F6-AC4)
        if topic_recommender is not None and any(
            kw in cleaned_text.lower() for kw in _TOPIC_KEYWORDS
        ):
            try:
                recommendation = await topic_recommender.recommend(user_id)
                await say(text=recommendation, thread_ts=thread_ts)  # type: ignore[operator]
            except Exception:
                logger.exception("Failed to generate topic recommendation")
                await say(  # type: ignore[operator]
                    text="ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€ãƒˆãƒ”ãƒƒã‚¯ææ¡ˆã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚",
                    thread_ts=thread_ts,
                )
            return

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ChatService ã§å¿œç­”
        try:
            response = await chat_service.respond(
                user_id=user_id,
                text=cleaned_text,
                thread_ts=thread_ts,
                channel=channel,
                is_in_thread=is_in_thread,
                current_ts=current_ts,
            )
            await say(text=response, thread_ts=thread_ts)  # type: ignore[operator]

            # ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±æŠ½å‡ºã‚’éåŒæœŸã§å®Ÿè¡Œ (F3-AC3)
            if user_profiler is not None:
                asyncio.create_task(
                    _safe_extract_profile(user_profiler, user_id, cleaned_text)
                )
        except Exception:
            logger.exception("Failed to generate response")
            await say(  # type: ignore[operator]
                text="ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€å¿œç­”ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãã—ã¦ã‹ã‚‰ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚",
                thread_ts=thread_ts,
            )

    @app.event("app_mention")
    async def handle_mention(event: dict, say: object) -> None:  # type: ignore[type-arg]
        user_id: str = event.get("user", "")
        text: str = event.get("text", "")
        raw_thread_ts: str | None = event.get("thread_ts")
        event_ts: str = event.get("ts", "")
        thread_ts: str = raw_thread_ts or event_ts
        files: list[dict[str, object]] | None = event.get("files")
        channel: str = event.get("channel", "")

        cleaned_text = strip_mention(text)
        if not cleaned_text:
            return

        await _process_message(
            user_id, cleaned_text, thread_ts, say, files,
            channel=channel,
            is_in_thread=raw_thread_ts is not None,
            current_ts=event_ts,
        )

    @app.event("message")
    async def handle_message(event: dict, say: object) -> None:  # type: ignore[type-arg]
        """è‡ªå‹•è¿”ä¿¡ãƒãƒ£ãƒ³ãƒãƒ«ã§ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å‡¦ç† (F6).

        ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° (F6-AC2, AC3, AC6, AC7):
        - bot_id ãŒã‚ã‚‹ â†’ ç„¡è¦–ï¼ˆBotè‡ªèº«ã®æŠ•ç¨¿ï¼‰
        - subtype ãŒã‚ã‚‹ â†’ ç„¡è¦–ï¼ˆç·¨é›†ã€å‰Šé™¤ãªã©ï¼‰
        - channel ãŒ auto_reply_channels ã«å«ã¾ã‚Œãªã„ â†’ ç„¡è¦–
        - ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ä»˜ã â†’ ç„¡è¦–ï¼ˆapp_mention ã§å‡¦ç†ã•ã‚Œã‚‹ï¼‰
        """
        # F6-AC6: è‡ªå‹•è¿”ä¿¡ãƒãƒ£ãƒ³ãƒãƒ«ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯ç„¡è¦–
        if not auto_reply_channels:
            return

        # F6-AC2: Botè‡ªèº«ã®æŠ•ç¨¿ã¯ç„¡è¦–
        if event.get("bot_id"):
            return

        # F6-AC3: ã‚µãƒ–ã‚¿ã‚¤ãƒ—ä»˜ããƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆç·¨é›†ã€å‰Šé™¤ãªã©ï¼‰ã¯ç„¡è¦–
        if event.get("subtype"):
            return

        # F6-AC1: å¯¾è±¡ãƒãƒ£ãƒ³ãƒãƒ«ã®ã¿å‡¦ç†
        channel: str = event.get("channel", "")
        if channel not in auto_reply_channels:
            return

        text: str = event.get("text", "")

        # F6-AC7: ãƒ¡ãƒ³ã‚·ãƒ§ãƒ³ä»˜ããƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯ app_mention ã§å‡¦ç†ã•ã‚Œã‚‹ãŸã‚ã‚¹ã‚­ãƒƒãƒ—
        # strip_mention ã¨åŒã˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½¿ç”¨
        if re.search(r"<@[A-Za-z0-9]+>\s*", text):
            return

        user_id: str = event.get("user", "")
        # user_id ãŒç©ºã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãªã©ã®ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹å¯¾å¿œï¼‰
        if not user_id:
            return

        raw_thread_ts: str | None = event.get("thread_ts")
        event_ts: str = event.get("ts", "")
        thread_ts: str = raw_thread_ts or event_ts
        files: list[dict[str, object]] | None = event.get("files")

        cleaned_text = text.strip()
        if not cleaned_text:
            return

        logger.info("Processing auto-reply message in channel %s", channel)
        await _process_message(
            user_id, cleaned_text, thread_ts, say, files,
            channel=channel,
            is_in_thread=raw_thread_ts is not None,
            current_ts=event_ts,
        )


async def _safe_extract_profile(
    profiler: UserProfiler, user_id: str, message: str
) -> None:
    """ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«æŠ½å‡ºã‚’å®‰å…¨ã«å®Ÿè¡Œã™ã‚‹ï¼ˆä¾‹å¤–ã‚’ãƒ­ã‚°ã«è¨˜éŒ²ï¼‰."""
    try:
        await profiler.extract_profile(user_id, message)
    except Exception:
        logger.exception("Failed to extract user profile for %s", user_id)
