# 自動レビューの最適化 — レトロスペクティブ

## 概要

自動パイプラインのレビュー手段として PR Review Toolkit（PRKit）と GitHub Copilot Code Review を比較分析し、最適な構成を決定した。議論ベースで進め、データに基づいた意思決定を行った。

## 関連 Issue/PR

- Issue #386: 自動レビューの最適化（本議論のベース）
- Issue #265: PR Review Toolkit 継続運用方法（精度データ蓄積）
- Issue #374: 自動実装パイプライン全体のリファクタリング
- Issue #379: Copilot 連携凍結（クローズ済み）

---

## 2026-02-15: 方針決定

### 何を議論したか

**テーマ**: 自動パイプラインのレビュー手段として何を使うか

**背景**:

- Copilot レビュー連携を凍結方針（#379）としていた
- PRKit が自動パイプラインの唯一のレビュー手段になる見込みだった
- しかし PRKit の精度が構造的に低い（0〜25%）ことがデータで判明していた

**要件**: 「精度: 高い」かつ「サイクル: 止まらない」を両立させたい

### 議論の流れ

#### 1. PRKit vs Copilot の立ち位置整理

両者の構造的な違いを明確化した。

| | Copilot Code Review | PR Review Toolkit |
|--|---------------------|-------------------|
| 本質 | 汎用レビュアー | 専門観点の深掘り分析器 |
| 入力 | PR全体（diff + リポジトリコンテキスト） | 個別ファイルの変更差分 |
| 視野 | 複数ファイルを横断的に分析 | 各エージェントが担当ファイルだけ分析 |
| レビューコスト | $0（GitHub サブスク込み） | ~$1/回（claude-code-action） |
| 精度（実測） | 80% (4/5) | 0〜25%（ツールによる） |

精度の差は「プロンプトの問題」ではなく「入力情報の構造的な差」に起因する。

#### 2. PRKit の設計意図の発見

公式ドキュメントを調査した結果、PRKit は**ローカルで開発者が対話的に使うセルフレビューツール**として設計されていることが判明。

| | 設計意図 | うちの使い方 |
|--|---------|-------------|
| 実行場所 | ローカル（Claude Code内） | GitHub Actions |
| 対話性 | 開発者が観点を指定、補足できる | 自動で全エージェント一括実行 |
| コンテキスト | 開発者がプロジェクト文脈を補足 | PR差分だけ渡す |
| タイミング | コミット前のセルフレビュー | PR作成後の自動レビュー |

**精度が低い根本原因は、設計と使い方のミスマッチだった。**

#### 3. 採用経緯の振り返り

Zenn記事「[PR Review ToolkitのGitHub Actions自動化ガイド](https://zenn.dev/driller/articles/pr-review-toolkit-github-action)」がきっかけ。記事では GitHub Actions での自動レビュー利用を推奨し、専用 Action も公開していた。

しかし記事には精度データが一切なく、「動いた」の紹介にとどまっていた。実運用では「動く」と「使える」に大きなギャップがあった。

**教訓**: ツール紹介記事は設定方法と動作確認が中心で、精度・誤検出率のデータを伴わないことが多い。自動レビューツールの評価には実際の PR で精度を計測する期間が必要。

#### 4. 選択肢の評価と絞り込み

| アプローチ | 精度 | サイクル | 採否 |
|---|---|---|---|
| PRKit 自動レビュー | ✕ (0〜25%) | ○ (即時) | ✕ 不採用 |
| Copilot | ○ (実測80%) | △ (schedule問題) | **○ 採用** |
| 新規ツール（Qodo等） | ? (未検証) | ○ | ✕ 今回は除外 |
| claude-code-action 直接レビュー | ? (未検証) | ○ | ✕ 今回は除外 |

- **PRKit**: 自動レビューとしては不適格。誤検出の仕分けコスト > 正当な指摘の価値。自動修正との組み合わせは有害の可能性
- **新規ツール**: 調査・導入コストが先に来る。PoC としてはまず既存の仕組みで回すべき
- **Copilot**: 精度は実績あり。schedule 問題を技術的に解決すれば要件を満たす

#### 5. Copilot の schedule 問題の解決策

**問題**: `copilot-review-poll.yml`（5分おき schedule）が不安定で、Issue が滞留する。

**解決策**: schedule 廃止 → ワークフロー内 sleep 待機（30秒おきポーリング、最大10分タイムアウト）。

- API レート制限の心配なし（最大20回、上限5,000回/時間）
- Copilot が数分で返せば早期に次へ進む
- ランナー時間コストは運用してから判断

### うまくいったこと

#### 1. データに基づいた意思決定

Issue #265 で蓄積した精度データ（全ツールの正答率）が議論の土台になった。「なんとなく使えそう」ではなく「18.8%」「0%」という数字があることで、感情に流されず判断できた。

#### 2. 構造的な問題の特定

「PRKit のプロンプトを改善すれば精度が上がる」という表面的な対策ではなく、「設計意図と使い方のミスマッチ」という構造的な問題を特定できた。これにより「改善で届くのか、構造的に無理なのか」の判断が明確になった。

#### 3. 外部情報の活用

- Manus による無料 AI コードレビューツール比較レポートで市場全体を俯瞰
- PRKit 公式ドキュメントで設計意図を確認
- 採用のきっかけとなった Zenn 記事を再評価

複数の情報源を突き合わせることで、偏りのない判断ができた。

### 改善点・ハマったこと

#### 1. 精度データの記録が不完全

- **問題**: 失敗（誤検出）のみ記録しており、成功（妥当な指摘）の体系的な記録がなかった
- **原因**: 記録テンプレートが未整備で、問題発生時のみアドホックに記録していた
- **対応**: 全件記録テンプレートを策定
- **教訓**: 精度の全体像を把握するには、成功事例も含めた体系的な記録が不可欠

```
PR# | ツール | 指摘内容(1行) | 妥当/誤検出 | 理由(1行)
```

#### 2. 設計意図の調査が後回しになった

- **問題**: PRKit を自動レビューに組み込んで精度問題に直面してから、ようやく「そもそもの設計意図」を調べた
- **原因**: Zenn 記事の紹介内容を鵜呑みにし、公式ドキュメントの推奨ユースケースを確認しなかった
- **対応**: 今回の議論で公式ドキュメントを調査し、設計意図とのミスマッチを特定
- **教訓**: ツール導入時は紹介記事だけでなく、公式ドキュメントの推奨ユースケースを必ず確認する

#### 3. Copilot 凍結方針との揺り戻し

- **問題**: Copilot 凍結（#379）を決めた直後に「やっぱり Copilot 一択」という結論に戻った
- **原因**: 凍結の理由（外部依存・コスト）と精度面の評価が別の軸であることを事前に整理していなかった
- **対応**: 精度データに基づき、Copilot を自動レビューの基盤として採用する方針に転換
- **教訓**: 方針決定時は判断軸（コスト、精度、安定性等）を分離して評価し、軸ごとの結論を明示しておく

### PRKit に対する評価の注意点

PRKit を「使えない」と断定するのは適切ではない。

- カスタマイズ（プロンプト改善、コンテキスト注入、エージェント選定）で改善できる余地はある
- ローカルのセルフレビューとしては有用（`/review-pr`、code-reviewer サブエージェント）
- 自動パイプラインという用途が設計と合っていなかっただけ

**評価**: 自動パイプラインに組み込んで運用する価値はない（現時点のリソースでは）。ローカル用途では引き続き活用する。

---

## 2026-02-15: sleep ポーリング方式の実装

### 何を実装したか

方針決定フェーズで策定した「schedule 廃止 → ワークフロー内 sleep ポーリング」を実装した。

**変更内容**:

| ファイル | 操作 | 概要 |
|---------|------|------|
| `wait-for-copilot.sh` | 新規 | Copilot レビュー検知スクリプト（30秒ポーリング） |
| `copilot-auto-fix.yml` | 大幅修正 | トリガー `labeled` → `opened` + `workflow_dispatch`、ポーリング組み込み、CI 待機全パス化 |
| `copilot-review-poll.yml` | 削除 | sleep ポーリングに移行のため不要 |
| `handle-errors.sh` | 修正 | 復旧手順メッセージを `workflow_dispatch` に更新 |

### うまくいったこと

#### 1. 仕様書先行のスムーズな実装

仕様書（PR #390）を先に完成させていたため、実装は仕様書のとおりに進められた。設計判断で迷う場面がなく、コードレビューでも仕様書との整合性確認がスムーズだった。

#### 2. 既存スクリプトの再利用

`_common.sh` の `require_env`, `output`, `validate_pr_number`, `validate_numeric` をそのまま活用できた。`copilot-review-poll.yml` の Copilot 検知ロジック（jq フィルタ）も `wait-for-copilot.sh` に移植できた。

#### 3. code-reviewer サブエージェントによるバグ検出

Merge check ステップの if 条件で CI タイムアウト時のガード漏れ（修正なしパスで CI タイムアウトしてもマージ判定に進む）を検出。`merge-check.sh` の `auto:failed` チェックで実害は防がれるが、防御的プログラミングの観点から修正した。PRKit なしでもセルフレビューが機能することを確認。

### 改善点・ハマったこと

#### 1. エムダッシュ（U+2014）の文字化け

シェルスクリプトのコメント内にエムダッシュ（`—`）を使っていたところ、shellcheck が UTF-8 エンコーディングエラーで失敗した。ASCII ダブルダッシュ（`--`）に置換して解決。

**教訓**: シェルスクリプト内では非 ASCII 文字（特に装飾用ダッシュ）を避ける。

#### 2. CRLF 改行コード（Windows 環境）

新規作成した `wait-for-copilot.sh` に CRLF 改行が混入。テスト実行時の自動変換（`tr -d '\r'`）で対応済みだが、Windows 環境での既知の問題。

---

## 次に活かすこと

### ツール導入の評価プロセス

1. **設計意図を先に確認する**: 公式ドキュメントの推奨ユースケースを読み、自分の用途と一致するか確認する
2. **紹介記事の精度データを確認する**: 「動いた」だけでは評価にならない。精度・誤検出率のデータがない場合は自分で計測する期間を設ける
3. **精度データは全件記録する**: 失敗のみの記録は偏りを生む。妥当な指摘も含めて体系的に記録する

### 自動レビューの設計原則

1. **「精度: 高い × サイクル: 止まらない」を両立させる**: どちらか一方では自動パイプラインとして機能しない
2. **セルフレビューツールと自動レビュアーは別物**: 対話的に使うツールを自動パイプラインに組み込んでも精度は出ない
3. **技術的な課題は構造的な限界より解決しやすい**: Copilot の schedule 問題（技術的）vs PRKit の精度問題（構造的）

### 意思決定のフレームワーク

1. **問題が交錯している時は分解する**: 精度の問題とサイクルの問題を分けて考えることで、選択肢が明確になった
2. **消去法で絞る**: 全選択肢を並べて、データに基づいて不採用を決めていくと、残った選択肢に集中できる
3. **方針転換を恐れない**: 凍結方針を出した直後でも、データが示す結論が異なるなら修正する
