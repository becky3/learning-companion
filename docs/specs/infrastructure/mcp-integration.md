# MCP 統合

## 概要

Model Context Protocol（MCP）を活用して、チャットボットが外部ツールを動的に発見・呼び出しできる基盤。MCP サーバーを追加するだけでボットの対応範囲を拡張できる。

サンプル実装として天気予報 MCP サーバーを含む。

## 背景

- LLM に対して定型的なプロンプトを送信し応答を得るのみで、外部ツールとの連携機能がなかった
- MCP（Anthropic が提唱、OpenAI・Linux Foundation も採用）はツール連携のオープン標準プロトコル
- MCP により、LLM がツールを動的に発見・呼び出しでき、エージェント的な振る舞いが可能になる

## 制約

### リポジトリ分離方針

MCP サーバーは独立プロセスとして動作するため、ホストアプリとのコード結合を避ける。

- MCP サーバーはトップレベルの `mcp_servers/` に配置する（`src/` の外）
- MCP サーバーのコードは `src/` 配下のモジュールを一切 import しない
- MCP サーバーとホストアプリは MCP プロトコルのみで通信する（直接の Python import 禁止）

### 機能制御

- `MCP_ENABLED` 環境変数で MCP 機能全体の ON/OFF を制御する
- MCP 無効時は従来どおりの動作をする（後方互換性）

### 安全弁

- ツール呼び出しループに最大反復回数を設ける。上限到達時はループを打ち切り、テキスト応答を強制する
- 個々のツール実行にタイムアウトを設ける。超過時はタイムアウトエラーとして処理し、エラー内容を LLM に返す

### ライフサイクル

- アプリ起動時に全サーバーへ接続する。失敗したサーバーはスキップし、ツールなしで続行する（グレースフルデグラデーション）
- サーバーの自動再起動・ヘルスチェックは行わない。初期実装ではシンプルさを優先し、障害回復はアプリ再起動で対応する
- アプリ終了時に全接続をクリーンアップし、子プロセスを停止する

### データ保存

- ツール呼び出しの中間ステップ（assistant + tool_calls、tool + 結果）は DB に保存しない。1 回の応答内で完結するため、スレッド再開時に再利用する必要がない

### トランスポート

- stdio（ローカルプロセス）のみサポートする。http（Streamable HTTP）は将来対応

## インターフェース

### MCP クライアント管理

ホストアプリが MCP サーバー群を管理するためのインターフェース。

| 操作 | 振る舞い |
| --- | --- |
| サーバー接続 | 設定ファイルに基づき全サーバーに接続。失敗したサーバーはスキップしてログ出力 |
| ツール一覧取得 | 全サーバーのツールをプロバイダー非依存の中間表現で返す |
| ツール実行 | 指定ツールを実行し結果を返す。ツール未発見時・実行失敗時は例外を送出 |
| システム指示取得 | 全サーバーの `system_instruction` を返す |
| 応答指示取得 | ツール名に対応するサーバーの `response_instruction` を返す |
| 自動コンテキストツール取得 | `auto_context_tool` が設定されたツール名のリストを返す |
| クリーンアップ | 全接続を閉じ、子プロセスを停止 |

### LLM プロバイダー拡張

既存の LLM 問い合わせに加え、ツール情報付きの問い合わせをサポートする。

| メソッド | 振る舞い |
| --- | --- |
| ツール付き問い合わせ | ツール定義リストを LLM に渡し、ツール呼び出し要求を含む応答を返す |

- 各プロバイダーがツール定義を自身の API 形式に変換する（OpenAI: Function Calling、Anthropic: Tool Use）
- ツール非対応プロバイダーはツールを無視して通常の問い合わせにフォールバックする

### MCP サーバー設定

JSON ファイルでサーバーの接続情報を管理する。

| フィールド | 説明 |
| --- | --- |
| `transport` | トランスポート種別（stdio のみ） |
| `command` | 実行コマンド |
| `args` | コマンド引数 |
| `env` | 環境変数 |
| `system_instruction` | システムプロンプトに常時追加する指示 |
| `response_instruction` | ツール実行後にシステムプロンプトへ追加する指示 |
| `auto_context_tool` | ユーザークエリで自動呼び出しし結果をコンテキスト注入するツール名 |

### ツール呼び出しフロー

1. ユーザーが質問する
2. チャットサービスが会話履歴 + 利用可能ツール情報を LLM に送信する
3. LLM が tool_use レスポンスを返す場合:
   - MCP クライアント経由でツールを実行する
   - ツール結果を会話履歴に追加し、LLM に再送信する
   - テキスト応答が得られるまで繰り返す（最大反復回数まで）
4. LLM がテキスト応答を返す場合、そのまま応答として返す

### 指示の適用タイミング

```mermaid
flowchart LR
    A["1. system_instruction<br/>（常時追加）"] --> B["2. auto_context 結果<br/>+ response_instruction"]
    B --> C["3. ツールループ内<br/>response_instruction<br/>（重複防止）"]
```

## コンポーネント構成

```mermaid
flowchart TB
    subgraph Host["MCP Host（AI Assistant）"]
        CS[チャットサービス]
        CM[MCP クライアント管理]
        LLM[LLM プロバイダー]
    end

    subgraph Servers["MCP サーバー群"]
        S1["天気予報サーバー"]
        S2["将来の拡張サーバー"]
    end

    CS --> CM
    CS --> LLM
    CM -->|stdio| S1
    CM -->|stdio| S2
    LLM -->|ツール定義| CS
```

| コンポーネント | 役割 |
| --- | --- |
| チャットサービス | ツール呼び出しループの実行、指示の注入 |
| MCP クライアント管理 | MCP サーバーへの接続管理、ツール一覧統合、ツール実行 |
| LLM プロバイダー | ツール定義の API 形式変換、ツール呼び出し判断 |
| MCP サーバー | 外部ツール・リソースの提供（MCP プロトコルで通信） |

## 外部連携

| 連携先 | プロトコル | 用途 |
| --- | --- | --- |
| MCP サーバー | MCP（stdio） | ツール発見・実行 |
| 気象庁 API（非公式） | HTTP | 天気予報データ取得（天気予報サーバーが使用） |

## エッジケース

| ケース | 振る舞い |
| --- | --- |
| サーバー接続失敗 | 該当サーバーをスキップし、他サーバーのツールのみで続行 |
| ツール実行エラー | エラー内容を LLM に伝え、LLM がユーザーに適切に回答 |
| ツールループ上限到達 | ループを打ち切り、上限到達メッセージを LLM に渡してテキスト応答を強制 |
| ツール実行タイムアウト | タイムアウトエラーとして処理し、エラー内容を LLM に返す |
| MCP 無効時 | 従来どおりの動作（ツールなしで応答） |
| 設定ファイル不在 | MCP 機能を無効として続行 |

## 関連ドキュメント

<!-- chat-response は未移行パスを含む。#648 で一括修正予定 -->
- [RAG ナレッジ](../infrastructure/rag-knowledge.md) — MCP サーバーとして動作する RAG 基盤
- [チャット応答](../features/chat-response.md) — ツール呼び出しループを含むチャット応答仕様
